<?xml version="1.0" encoding="UTF-8"?>

<configuration scan="true">
<!-- 	<include resource="org/springframework/boot/logging/logback/base.xml" /> -->

	

	<property name="logging.path" value="logs" />

	<appender name="STDOUT" class="ch.qos.logback.core.ConsoleAppender">
		<layout class="ch.qos.logback.classic.PatternLayout">
			<Pattern>
				%d{yyyy-MM-dd HH:mm:ss} [%thread] %-5level %logger{36} - %msg%n
			</Pattern>
		</layout>
	</appender>

	<appender name="CUSTOM"
		class="ch.qos.logback.core.rolling.RollingFileAppender">
		<file>${logging.path}/demo.log</file>
		<encoder class="ch.qos.logback.classic.encoder.PatternLayoutEncoder">
			<Pattern>
				%d{yyyy-MM-dd HH:mm:ss} - %msg%n
			</Pattern>
		</encoder>

		<rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
			<!-- rollover daily -->
			<fileNamePattern>${logging.path}/archived/demo.%d{yyyy-MM-dd}.%i.log
                        </fileNamePattern>
			<timeBasedFileNamingAndTriggeringPolicy
				class="ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP">
				<maxFileSize>10MB</maxFileSize>
			</timeBasedFileNamingAndTriggeringPolicy>
		</rollingPolicy>

	</appender>
	<appender name="kafkaAppender" class="com.github.danielwegener.logback.kafka.KafkaAppender">
            <!-- This is the default encoder that encodes every log message to an utf8-encoded string  -->
            <encoder class="com.github.danielwegener.logback.kafka.encoding.LayoutKafkaMessageEncoder">
                <layout class="ch.qos.logback.classic.PatternLayout">
                    <pattern>%d{HH:mm:ss.SSS} [%thread] %-5level %logger{36} - %msg%n</pattern>
                </layout>
            </encoder>
            <topic>test</topic>
            <keyingStrategy class="com.github.danielwegener.logback.kafka.keying.RoundRobinKeyingStrategy" />
            <deliveryStrategy class="com.github.danielwegener.logback.kafka.delivery.AsynchronousDeliveryStrategy" />

            <!-- each <producerConfig> translates to regular kafka-client config (format: key=value) -->
            <!-- producer configs are documented here: https://kafka.apache.org/documentation.html#newproducerconfigs -->
            <!-- bootstrap.servers is the only mandatory producerConfig -->
            <producerConfig>bootstrap.servers=localhost:9092</producerConfig>

            <!-- this is the fallback appender if kafka is not available. -->
            <appender-ref ref="STDOUT"/>
        </appender>

   <!--  <root level="info">
        <appender-ref ref="kafkaAppender" />
    </root>
 -->
	
	<logger name="com.demo.TestKafka" level="info"
		additivity="false">
			<appender-ref ref="kafkaAppender" />

	</logger>

	<appender name="ERROR"
		class="ch.qos.logback.core.rolling.RollingFileAppender">
		<file>${logging.path}/demo_error.log</file>
		<encoder class="ch.qos.logback.classic.encoder.PatternLayoutEncoder">
			<Pattern>
				%d{yyyy-MM-dd HH:mm:ss} - %msg%n
			</Pattern>
		</encoder>

		<rollingPolicy class="ch.qos.logback.core.rolling.TimeBasedRollingPolicy">
			<!-- rollover daily -->
			<fileNamePattern>${logging.path}/archived/demo_error.%d{yyyy-MM-dd}.%i.log
                        </fileNamePattern>
			<timeBasedFileNamingAndTriggeringPolicy
				class="ch.qos.logback.core.rolling.SizeAndTimeBasedFNATP">
				<maxFileSize>10MB</maxFileSize>
			</timeBasedFileNamingAndTriggeringPolicy>
		</rollingPolicy>

	</appender>
		<!-- Data is written to the destination - recording> -->
	<appender name="queue" class="ch.qos.logback.classic.AsyncAppender">
		<discardingThreshold>0</discardingThreshold>
	      <queueSize>10000</queueSize>
	      <appender-ref ref="CUSTOM" />
	</appender>

	<!-- Send logs to both console and file audit -->
	
	<!--Controller - NPCI XML request , response > -->
	<logger name="com.demo" level="info"
		additivity="false">
		<appender-ref ref="queue" />
	</logger>

	

	<root level="error">
		<appender-ref ref="ERROR" />
	</root>
	<contextListener class="ch.qos.logback.classic.jul.LevelChangePropagator">
		<resetJUL>true</resetJUL>
	</contextListener>

</configuration>